{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latin-share",
   "metadata": {},
   "source": [
    "<center><h1>Hands-On Reinforcement Learning Applied to Trade Execution Algorithms</h1></center>\n",
    "<center>\n",
    "Autor: <cite><a href=\"https://www.linkedin.com/in/aoteog/\">Oteo García, Alberto</a></cite>\n",
    "</center>\n",
    "<center>\n",
    "Autor: <cite><a href=\"https://www.linkedin.com/in/jesus-sanz/\">Sanz del Real, Jesús</a></cite>\n",
    "</center>\n",
    "\n",
    "----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from collections import  deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "from agents.dqn import DDQNAgent\n",
    "from report.report import plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/rep_data.pickle\", \"rb\") as f:\n",
    "        dict_ = pickle.load(f)\n",
    "data = dict_[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-usage",
   "metadata": {},
   "source": [
    "---\n",
    "# Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestExecutionEnv:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data, look_back=60):\n",
    "        \"\"\"Inicialización de la clase del entorno que simula\n",
    "        el libro de ordenes.\n",
    "        ----------------------------------------------------\n",
    "        Input:\n",
    "            - data: \n",
    "                Dataframe con los datos previamente\n",
    "                agrupados del libro de órdenes.\n",
    "            \n",
    "            - look_back: \n",
    "                Ventana para la generación de features\n",
    "                roladas en el instante t=0 del episodio.\n",
    "                Esta ventana representa el rango máximo para\n",
    "                la construcción de features.\n",
    "        ----------------------------------------------------\n",
    "        Variables Internas:\n",
    "            - episode_bins:\n",
    "                Número de bines (steps) del episodio.\n",
    "            - episode_full_len:\n",
    "                Es igual a look_back + episode_bins.\n",
    "            - vol_care:\n",
    "                Volumen total (en títulos) de la orden care.\n",
    "            - actions_fn:\n",
    "                Diccionario con las posibles acciones del agente.\n",
    "                Las claves acceden a la función que evalúa  la acción\n",
    "                tomada por el agente.\n",
    "            - n_actions:\n",
    "                Número de acciones posibles.\n",
    "            - n_features:\n",
    "                Número de características de los estados.\n",
    "            - episode:\n",
    "                Dataframe que contiene los steps y estados del episodio.\n",
    "            - episode_full:\n",
    "                Es el episode añadiendo el look_back antes del comienzo \n",
    "                del episodio.\n",
    "            - episode_vwap:\n",
    "                VWAP de mercado al final del episodio.\n",
    "            - market_ep_vol:\n",
    "                Volumen (títulos) ejecutado por el mercado en cada bin del episodio.\n",
    "            - state_pos:\n",
    "                Número de step en el que nos encontramos.\n",
    "            - exec_vol:\n",
    "                Acumulado de títulos ejecutados por el algoritmo.\n",
    "            - action_hist:\n",
    "                Lista de acciones tomadas por el algoritmo en cada step.\n",
    "            - market_vwap_hist:\n",
    "                Lista de VWAP de mercado en cada step.\n",
    "            - reward_hist:\n",
    "                Lista de rewards obtenidas en cada step.\n",
    "            - price_hist:\n",
    "                Lista de precios ejecutados en cada step.\n",
    "            - vol_hist:\n",
    "                Lista de títulos ejecutados en cada step.                \n",
    "        \"\"\"\n",
    "        \n",
    "        # Fixed params\n",
    "        self.data = data\n",
    "        self.look_back = look_back\n",
    "        self.episode_bins = None\n",
    "        self.episode_full_len = None\n",
    "        self.vol_care = None\n",
    "        \n",
    "        self.actions_fn = {\n",
    "            0: self._do_nothing,\n",
    "            1: self._agg_action,\n",
    "        }\n",
    "        \n",
    "        self.n_actions = len(self.actions_fn)\n",
    "        self.n_features = self._detect_num_feat()\n",
    "\n",
    "        # Data variables\n",
    "        self.episode = None\n",
    "        self.episode_full = None\n",
    "        \n",
    "        # Env variables\n",
    "        self.episode_vwap = None\n",
    "        self.market_ep_vol = None\n",
    "        self.state_pos = 0\n",
    "        self.exec_vol = 0\n",
    "        self.actions_hist = []\n",
    "        self.algo_vwap_hist = []\n",
    "        self.market_vwap_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.price_hist = []\n",
    "        self.vol_hist = []\n",
    "        \n",
    "    def _detect_num_feat(self):\n",
    "        \"\"\"Detecta el número de variables del estado.\n",
    "        Función necesaria para adaptarse automaticamente a los\n",
    "        cambios en las variables de observation_builder.\n",
    "        \"\"\"\n",
    "        self._reset_env_episode_params()\n",
    "        self._generate_episode()\n",
    "        s = self.observation_builder()\n",
    "        return s.shape[0]\n",
    "        \n",
    "    def _reset_env_episode_params(self):\n",
    "        \"\"\"\n",
    "        Reset del episodio e inicialización de los parámetros.\n",
    "        Las variables internas vuelven a sus valores originales.\n",
    "        \"\"\"\n",
    "        self.episode_full_len = None\n",
    "        self.episode = None\n",
    "        self.episode_full = None\n",
    "        self.episode_vwap = None\n",
    "        self.market_ep_vol = None\n",
    "        self.state_pos = 0\n",
    "        self.algo_vwap = 0\n",
    "        self.exec_vol = 0\n",
    "        self.actions_hist = []\n",
    "        self.algo_vwap_hist = []\n",
    "        self.market_vwap_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.price_hist = []\n",
    "        self.vol_hist = []\n",
    "        \n",
    "    def _generate_episode_params(self):\n",
    "        \"\"\"Se determinan las características de la orden a ejecutar.\n",
    "        La órden queda definida por: \n",
    "         - episode_bins:\n",
    "             Obtención de un número entero aleatorio [400, 600] \n",
    "             con una distribución uniforme.\n",
    "         - vol_care:\n",
    "             Obtención del porcentaje de steps en el que hay que \n",
    "             ejecutar una órden para cubrir la órden care. \n",
    "             vol_care responde a un valor uniforme [0.075, 0.125]\n",
    "             multiplicado por el número self.episode_bins. \n",
    "             Lo convertimos a entero.\n",
    "        \"\"\"\n",
    "        # TODO: Int aleatorio entre 400 y 600 como un objeto numpy\n",
    "        self.episode_bins = np.random.randint(low=400, high=600)\n",
    "        # TODO: Float aleatorio entre 0.075 y 0.125\n",
    "        pct_bins = np.random.uniform(low=0.075, high=0.125)\n",
    "        # TODO: Int multiplicacion pct_bins y episode_bins\n",
    "        self.vol_care = int(pct_bins * self.episode_bins)\n",
    "        \n",
    "        self.episode_full_len = self.episode_bins + self.look_back\n",
    "        \n",
    "        assert self.episode_bins <= 600\n",
    "        assert self.episode_bins >= 400\n",
    "        assert self.vol_care <= int(self.episode_bins * 0.125)\n",
    "        assert self.vol_care >= int(self.episode_bins * 0.075)\n",
    "        assert isinstance(self.vol_care, int)\n",
    "        \n",
    "    def _generate_episode(self):\n",
    "        \"\"\"Obtenemos el día y hora en el que comienza el episodio.\n",
    "        \"\"\"\n",
    "        self._generate_episode_params()\n",
    "        \n",
    "        lenght_episode = 0\n",
    "        while lenght_episode != self.episode_full_len:\n",
    "            # TODO: Selección de un dia entre los posibles.\n",
    "            # Clue: Usa np.random.choice y los dias data.keys\n",
    "            selected_day = np.random.choice(\n",
    "                    list(self.data.keys()\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # TODO: Extrae selected_day de data\n",
    "            data_day = self.data[selected_day]\n",
    "            \n",
    "            # TODO: selecciona una hora de inicio aleatoria\n",
    "            init_time = np.random.choice(data_day.index)\n",
    "            \n",
    "            hour_pos = data_day.index.get_loc(init_time)\n",
    "            initial_position = hour_pos - self.look_back\n",
    "            final_position = hour_pos + self.episode_bins\n",
    "            \n",
    "            if initial_position < 0:\n",
    "                continue\n",
    "            else:\n",
    "                # TODO: Filtra data_day entre por initial_position y final_position\n",
    "                self.episode_full = data_day.iloc[initial_position:final_position, :]\n",
    "                \n",
    "                # TODO: Filtra data_day entre por hour_pos y final_position\n",
    "                self.episode = data_day.iloc[hour_pos:final_position, :]\n",
    "                \n",
    "                lenght_episode = self.episode_full.shape[0]\n",
    "        \n",
    "            \n",
    "    def reset(self) -> np.array:\n",
    "        \"\"\"Reinicialización del episodio junto con los parámetros.\n",
    "        Devuelve la primera observación del nuevo episodio.\n",
    "        \"\"\"  \n",
    "        self._reset_env_episode_params()     \n",
    "        self._generate_episode()\n",
    "        self._compute_episode_market_feat()\n",
    "        \n",
    "        return self.observation_builder()\n",
    "    \n",
    "    def observation_builder(self) -> np.array:\n",
    "        \"\"\" Función para la construcción de las observaciones del estado.\n",
    "            ------------------------------------------------------------\n",
    "            Default:\n",
    "                - Primera característica es tiempo restante en porcentaje.\n",
    "                - Seguna característica es el volumen restante en porcentaje.\n",
    "        \"\"\"\n",
    "        # TODO: Construye el vector con las dos características de la descripción\n",
    "        # Clue: Utiliza episode_bins, state_pos, exec_vol ,vol_care\n",
    "        time_left = (self.episode_bins - self.state_pos) / self.episode_bins\n",
    "        vol_left = 1 - (self.exec_vol / self.vol_care)\n",
    "        obs  = np.array([time_left, vol_left])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _compute_episode_market_feat(self) -> Tuple[float, float]:\n",
    "        \"\"\"Cálculo de los valores VWAP y Market Vol del episodio.\n",
    "        Como no tenemos las ejecuciones de mercado, asumimos que el \n",
    "        precio es el mid price de cada step.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula el mid price utilizando ask1 y bid1 de episode\n",
    "        # Opcional: Utiliza un precio más realista para el mkt VWAP\n",
    "        mid = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        # TODO: Calcula market_ep_vol\n",
    "        self.market_ep_vol = self.episode.cumvol.diff()\n",
    "        self.market_ep_vol[0] = 0\n",
    "        # TODO: calcula el volumen acumulado del mercado en todo el episodio\n",
    "        cum_vol = self.market_ep_vol.sum()\n",
    "        # TODO: calcula el episode_vwap\n",
    "        self.episode_vwap = (mid[:-1] * self.market_ep_vol[1:]).sum() / cum_vol\n",
    "        \n",
    "        return self.episode_vwap, self.market_ep_vol\n",
    "    \n",
    "    def _compute_algo_vwap(self) -> float:\n",
    "        \"\"\"Cálculo del VWAP del algoritmo hasta el step actual.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula el algo_vwap\n",
    "        # Clue: utiliza price_hist, vol_hist\n",
    "        p_arr = np.array(self.price_hist)\n",
    "        v_arr = np.array(self.vol_hist)\n",
    "        algo_vwap = np.sum(p_arr * v_arr) / np.sum(v_arr)\n",
    "        return algo_vwap\n",
    "    \n",
    "    def _compute_reward(self, price: float, vol: float) -> float:\n",
    "        \"\"\"Función de diseño de los rewards y penalizaciónes que \n",
    "        recibe el algoritmo al tomar las acciones.\n",
    "        --------------------------------------------------------\n",
    "        Default:\n",
    "            - El reward es el ratio de la diferencia entre el episode_vwap y\n",
    "              el precio de la acción tomada, dividido entre episode_vwap.\n",
    "        \"\"\"\n",
    "        # TODO: Establece y devuelve un reward cuando vol == 0\n",
    "        if vol == 0:\n",
    "            reward = 0\n",
    "            return reward\n",
    "        # TODO: Calcula y devuelve el reward cuando vol > 0\n",
    "        # Clue: Utiliza episode_vwap y price para la reward por defecto\n",
    "        # Opcional: Utiliza el self y elimina los parámetros de la función\n",
    "        reward = (self.episode_vwap - price) / self.episode_vwap\n",
    "        return reward\n",
    "    \n",
    "    def _compute_stop_conditions(self) -> Tuple[bool, bool]:\n",
    "        \"\"\"Define las condiciones de parada del episodio\n",
    "        Return:\n",
    "            Tiempo agotado, orden completada\n",
    "        \"\"\"\n",
    "        # TODO: Calcula las variables de parada y devuélvelas en el orden apropiado\n",
    "        is_bins_complete = self.state_pos == self.episode_bins\n",
    "        is_ord_complete = self.exec_vol == self.vol_care\n",
    "        return is_bins_complete, is_ord_complete\n",
    "    \n",
    "    def _compute_done_reward(self) -> float:\n",
    "        # TODO: Free style\n",
    "        _, is_ord_complete = self._compute_stop_conditions()\n",
    "        rwd_factor = not is_ord_complete\n",
    "        done_reward = -1 * rwd_factor\n",
    "        return done_reward\n",
    "    \n",
    "    def _agg_action(self) -> float:\n",
    "        \"\"\"Acción agresiva de compra de un título a precio de episode['ask1'].\n",
    "        Devolvemos el reward asociado a esa acción.\n",
    "        \"\"\"\n",
    "        # TODO: obtén el precio de la accion agresiva (ask1) en el state_pos\n",
    "        price = self.episode[\"ask1\"].values[self.state_pos]\n",
    "        # TODO: guarda price en price_hist, añade 1 a exec_vol y añade 1 a vol_hist\n",
    "        self.price_hist.append(price)\n",
    "        exec_vol = 1\n",
    "        self.exec_vol += exec_vol\n",
    "        self.vol_hist.append(exec_vol)\n",
    "        \n",
    "        # TODO: utiliza la función apropiada para calcula el algo_vwap\n",
    "        algo_vwap = self._compute_algo_vwap()\n",
    "        # guarda el algo_vwap en algo_vwap_hist\n",
    "        self.algo_vwap_hist.append(algo_vwap)\n",
    "        # TODO: calcula el reward utilizando la función apropiada\n",
    "        reward = self._compute_reward(price, exec_vol)\n",
    "        return reward\n",
    "\n",
    "    def _do_nothing(self) -> float:\n",
    "        \"\"\"No hacer nada y devolvemos el reward asociado a la acción\n",
    "        \"\"\"\n",
    "        # TODO: Repite el proceso de _agg_action\n",
    "        # Clue: Precio y volumen ejecutado = 0\n",
    "        price = 0\n",
    "        exec_vol = 0\n",
    "        self.price_hist.append(price)\n",
    "        self.vol_hist.append(exec_vol)\n",
    "        algo_vwap = self.algo_vwap_hist[-1]\n",
    "        self.algo_vwap_hist.append(algo_vwap)\n",
    "        reward = self._compute_reward(price, exec_vol)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _compute_market_vwap(self) -> float:\n",
    "        \"\"\"Cálculo del VWAP del mercado hasta el step actual.\n",
    "        \"\"\"\n",
    "        # TODO: Establece un para el vol ejecutado por el mkt en cada step\n",
    "        # Clue: puedes fijarte en _compute_episode_market_feat\n",
    "        mid_p = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        mkt_p = (mid_p + mid_p.shift(-1).ffill()) / 2\n",
    "        # Calcula todos los vwap del mkt hasta el step actual incluido\n",
    "        v = self.episode[\"cumvol\"].diff().shift(-1)\n",
    "        p_arr = mkt_p.values[:self.state_pos + 1]\n",
    "        v_arr = v.values[:self.state_pos + 1]\n",
    "        sum_vol = np.sum(v_arr)\n",
    "        # Si el mkt vol hasta el step == 0, devuelve el último precio hasta el step\n",
    "        if sum_vol == 0:\n",
    "            return p_arr[-1]\n",
    "        # Calcula y devuelve el vwap acumulado hasta el step\n",
    "        market_vwap = np.sum(p_arr * v_arr) / sum_vol\n",
    "        return market_vwap\n",
    "    \n",
    "    def _compute_done(self) -> bool:\n",
    "        \"\"\" Reglas de finalización del episodio.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula las condiciones de parada utilizando la función adecuada\n",
    "        conditions = self._compute_stop_conditions()\n",
    "        is_bins_complete = conditions[0]\n",
    "        is_ord_complete = conditions[1]\n",
    "        # TODO: Devuelve done == True si se cumplen cualquiera de las condiciones\n",
    "        done = is_bins_complete or is_ord_complete\n",
    "        return done\n",
    "\n",
    "    def step(self, action) -> Tuple[np.array, float, bool, dict]:\n",
    "        \"\"\" Evalua la acción, calcula la recompensa, devuelve el \n",
    "        nuevo estado y si el episodio ha terminado.\n",
    "        \"\"\"\n",
    "        \n",
    "        market_vwap = self._compute_market_vwap()\n",
    "        act_fn = self.actions_fn.get(action)\n",
    "        if act_fn is None:\n",
    "            raise ValueError(\n",
    "                f\"Invalid action {action}. Valid actions {self.actions_fn.keys()}\"\n",
    "            )\n",
    "        \n",
    "        reward = act_fn()\n",
    "\n",
    "        self.market_vwap_hist.append(market_vwap)\n",
    "        self.reward_hist.append(reward)\n",
    "        \n",
    "        self.state_pos += 1\n",
    "        \n",
    "        done = self._compute_done()\n",
    "        \n",
    "        if done:\n",
    "            reward += self._compute_done_reward()\n",
    "            return None, reward, done, {}\n",
    "        \n",
    "        observation = self.observation_builder()\n",
    "        \n",
    "        return np.array(observation), reward, done, {}\n",
    "    \n",
    "    def action_sample(self) -> int:\n",
    "        \"\"\"\n",
    "        Devuelve una acción aleatoria. El valor ha de corresponder \n",
    "        con las keys de actions_fn.\n",
    "        \"\"\"\n",
    "        # TODO: Toma una acción aleatoria\n",
    "        # Opcional: ¿Qué distribución de prob es mejor para la exploración?\n",
    "        p = self.vol_care / self.episode.shape[0]\n",
    "        action = np.random.choice([0, 1], p=[1-p, p])\n",
    "        return action\n",
    "\n",
    "    def stats_df(self):\n",
    "        \"\"\"Información para el gráfico de resultados de la ejecución\n",
    "        \"\"\"\n",
    "        \n",
    "        my_df = pd.DataFrame(\n",
    "            {\"vwap\": self.algo_vwap_hist, \"vol\": self.vol_hist},\n",
    "            index=list(self.episode.index)[:len(self.algo_vwap_hist)]\n",
    "        )\n",
    "        my_df = my_df.reindex(self.episode.index)\n",
    "        my_df[\"vol\"] = my_df[\"vol\"].fillna(0)\n",
    "        my_df[\"vwap\"] = my_df[\"vwap\"].ffill()\n",
    "            \n",
    "        \n",
    "        p = self.episode[\"ask1\"]\n",
    "        v = self.episode[\"cumvol\"].diff().shift(-1)\n",
    "        last_v = self.episode_full[\"cumvol\"].diff()[-1]\n",
    "        v.iloc[-1] = last_v\n",
    "        market_vwap = (p * v).cumsum() / v.cumsum()\n",
    "        market_df = pd.DataFrame(\n",
    "            {\"vwap\": market_vwap, \"vol\": v},\n",
    "            index=v.index\n",
    "        )\n",
    "        \n",
    "        mpx = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        \n",
    "        return my_df, market_df, mpx\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TWAP(DDQNAgent):\n",
    "    def act(self, s):\n",
    "        # TODO: Configura un TWAP determinista utilizando s\n",
    "        if s[1] >= s[0]:\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Agent Params\n",
    "\"\"\"\n",
    "# TODO: Selecciona un epsilon inicial para el entrenamiento\n",
    "epsilon = 1\n",
    "# TODO: Selecciona un min_epsilon para el entrenamiento\n",
    "min_epsilon = 0.05\n",
    "# TODO: Selecciona un gamma para el aprendizaje\n",
    "gamma = 1\n",
    "# TODO: Selecciona un alpha para el aprendizaje\n",
    "alpha = 0.0001\n",
    "# TODO: Selecciona un buffer_size para el aprendizaje\n",
    "buffer_size = 80000\n",
    "# TODO: Selecciona un batch_size para el aprendizaje\n",
    "batch_size = 256\n",
    "# TODO: Selecciona el número de nueronas para el modelo\n",
    "hidden_neurons = 240\n",
    "\n",
    "\"\"\"\n",
    "    Training Params\n",
    "\"\"\"\n",
    "# TODO: Selecciona el número de episodios\n",
    "nepisodes = 1000\n",
    "n_log = 25\n",
    "#TODO: Determina el epsilon_decay para el proceso de entrenamiento\n",
    "epsilon_decay = (epsilon - min_epsilon) / (nepisodes * 0.95)\n",
    "learn_after = batch_size\n",
    "\n",
    "env = BestExecutionEnv(data, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-holocaust",
   "metadata": {},
   "source": [
    "#### Inicialización del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(\n",
    "    env, gamma=gamma, epsilon=epsilon, alpha=alpha,\n",
    "    batch_size=batch_size, buffer_size=buffer_size,\n",
    "    hidden_neurons=hidden_neurons, trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-request",
   "metadata": {},
   "source": [
    "#### Recolección de los Episodios para el Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto eps es 1 -> actuando random\n",
    "s = env.reset()\n",
    "for exps in range(buffer_size):  \n",
    "    a = agent.act(s)\n",
    "    s1, r, done, _ = env.step(a)\n",
    "    agent.experience(s, a, r, s1, done)\n",
    "    s = s1\n",
    "\n",
    "    if not exps % 10000:\n",
    "        print(f'buffer exps: {exps}')\n",
    "    if done:\n",
    "        s = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-brown",
   "metadata": {},
   "source": [
    "#### Train Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_trainable(True)\n",
    "learn_counter = 0\n",
    "history_steps = []\n",
    "history_rewards = []\n",
    "history_disc_rewards = []\n",
    "history_losses = []\n",
    "\n",
    "list_df = []\n",
    "list_market_df = []\n",
    "list_mpx = []\n",
    "\n",
    "for episode in range(nepisodes):\n",
    "    s = env.reset()\n",
    "    step = 0\n",
    "    cum_reward = 0\n",
    "    dis_cum_reward = 0\n",
    "    episode_losses = []\n",
    "    while True:\n",
    "        a = agent.act(s)\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        agent.experience(s, a, r, s1, done)\n",
    "        learn_counter += 1\n",
    "        cum_reward += r\n",
    "        dis_cum_reward += agent.gamma ** step * r\n",
    "        s = s1\n",
    "        step += 1\n",
    "        if not learn_counter % learn_after:\n",
    "            mse = agent.learn()\n",
    "        if done:\n",
    "            agent.epsilon = max([agent.epsilon - epsilon_decay, min_epsilon])\n",
    "            history_rewards.append(cum_reward)\n",
    "            history_disc_rewards.append(dis_cum_reward)\n",
    "            history_losses.append(mse)\n",
    "            history_steps.append(step)\n",
    "            \n",
    "            res = env.stats_df()\n",
    "            list_df.append(res[0])\n",
    "            list_market_df.append(res[1])\n",
    "            list_mpx.append(res[2])\n",
    "            \n",
    "            if not episode % n_log:\n",
    "                mse = agent.learn()\n",
    "                print(\n",
    "                    f'Episode: {episode}, '\n",
    "                    f'steps: {np.round(np.mean(history_steps[-n_log:]), 2)}, '\n",
    "                    f'rew: {np.round(np.mean(history_rewards[-n_log:]), 2)}, '\n",
    "                    f'mse: {np.round(mse)}, '\n",
    "                    f'eps: {np.round(agent.epsilon, 2)}'\n",
    "                )\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-visibility",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_trainable(False)\n",
    "cum_reward = 0\n",
    "step = 0\n",
    "env = BestExecutionEnv(df[\"test\"], 60)\n",
    "s = env.reset()\n",
    "a = 1\n",
    "s, r, done, _ = env.step(a)\n",
    "step += 1\n",
    "cum_reward += agent.gamma ** step * r\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    s, r, done, _ = env.step(a)\n",
    "    step += 1\n",
    "    cum_reward += agent.gamma ** step * r\n",
    "    if done:\n",
    "        break\n",
    "plot_results(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-springer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
