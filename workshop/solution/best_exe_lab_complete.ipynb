{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latin-share",
   "metadata": {},
   "source": [
    "<center><h1>Aprendizaje por Refuerzo Hands-On</h1></center>\n",
    "<center>\n",
    "Autor: <cite><a href=\"https://www.linkedin.com/in/aoteog/\">Oteo García, Alberto</a></cite>\n",
    "</center>\n",
    "<center>\n",
    "Autor: <cite><a href=\"https://www.linkedin.com/in/jesus-sanz/\">Sanz del Real, Jesús</a></cite>\n",
    "</center>\n",
    "\n",
    "----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-ranch",
   "metadata": {},
   "source": [
    "<center><h1>Summary</h1></center>\n",
    "\n",
    "<center>\n",
    "\n",
    "<p style=\"width:80%\">\n",
    "    En este notebook se presenta el taller de aprendizaje por refuerzo aplicado a un caso real para finanzas. El objetivo es plantear la resolución del problema de <b>mejor ejecución</b> a través del uso\n",
    "    de técnicas de aprendizaje por refuerzo desde cero. Para ello, durante el taller se ha de completar el entorno básico <b>BestExecutionEnv</b> donde los asistentes desarrollarán las partes claves del\n",
    "    código para obtener un simulador de mercado que, aunque relativamente naive, será funcional. Posteriormente, los participantes tendrán de aplicar algoritmos clasicos y basados en inteligencia artificial (IA) para la\n",
    "    resolución del problema. \n",
    "</p>\n",
    "<p style=\"width:80%\">\n",
    "    En términos sencillos el problema de <b>mejor ejecución</b> trata sobre la ejecución de órdenes con un tamaño suficiente como para que su ejecución suponga un <b><i>'problema'</i></b> como se comenta durante la clase 6 de máster. La resolución de la ejecución de estas órdenes (ordenes CARE) se puede realizar utilizando diferentes aproximaciónes. En el caso de este taller, queremos optimizar el <b><i>timing</i></b> de nuestras órdenes para minimizar el impacto de mercado, es decir, buscaremos trocear la orden CARE para comprar (vender) a los menores precios (mayores precios) de la sesión en la que se ejecuta la orden CARE. Como baseline para saber si nuestra ejecución es buena o mala utilizaremos el baseline habitual en el sector, el VWAP:\n",
    "</p>   \n",
    "$$\\frac{\\sum_{i=1}^T p_i\\cdot v_i}{\\sum_{i=1}^Tv_i}$$\n",
    "\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-favor",
   "metadata": {},
   "source": [
    "<center><h1>Datos</h1></center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "Los datos para este taller se han extraido tomando como referencia los activos Repsol y Santander. Para mantener la simplicidad del problema contamos con la primera posicion del libro de ordenes tanto el bid como del ask y el volumen acumulado agrupado por 5 segundos. El tiempo del libro de ordenes comprende las 13:00 y las 17:25 de los días bursatiles seleccionados. El conjunto de datos esta partindo en 3 partes:\n",
    "</p>\n",
    "    \n",
    "</center> \n",
    "<center>\n",
    "    <p style=\"width:30%\"><b>Train</b>: Primeros 60 días hábiles de 2018</p>\n",
    "  <p style=\"width:30%\"><b>Validación</b>: Primeros 30 días hábiles de 2019</p>\n",
    "  <p style=\"width:30%\"><b>Test</b>: Intevalo de los días hábiles del 31 al 60 de 2019</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from collections import  deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "from agents.dqn import DDQNAgent\n",
    "from report.report import plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/rep_data.pickle\", \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "data = df[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-usage",
   "metadata": {},
   "source": [
    "---\n",
    "# Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestExecutionEnv:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data, look_back=60):\n",
    "        \"\"\"Inicialización de la clase del entorno que simula\n",
    "        el libro de ordenes.\n",
    "        ----------------------------------------------------\n",
    "        Input:\n",
    "            - data: \n",
    "                Dataframe con los datos previamente\n",
    "                agrupados del libro de órdenes.\n",
    "            \n",
    "            - look_back: \n",
    "                Ventana para la generación de features\n",
    "                roladas en el instante t=0 del episodio.\n",
    "                Esta ventana representa el rango máximo para\n",
    "                la construcción de features.\n",
    "        ----------------------------------------------------\n",
    "        Variables Internas:\n",
    "            - episode_bins:\n",
    "                Número de bines (steps) del episodio.\n",
    "            - episode_full_len:\n",
    "                Es igual a look_back + episode_bins.\n",
    "            - vol_care:\n",
    "                Volúmen total (en títulos) de la orden care.\n",
    "            - actions_fn:\n",
    "                Diccionario con las posibles acciones del agente.\n",
    "                Las claves acceden a la función que evalúa  la acción\n",
    "                tomada por el agente.\n",
    "            - n_actions:\n",
    "                Número de acciones posibles.\n",
    "            - n_features:\n",
    "                Número de características de los estados.\n",
    "            - episode:\n",
    "                Dataframe que contiene los steps y estados del episodio.\n",
    "            - episode_full:\n",
    "                Es el episode añadiendo el look_back antes del comienzo \n",
    "                del episodio.\n",
    "            - episode_vwap:\n",
    "                VWAP de mercado al final del episodio.\n",
    "            - market_ep_vol:\n",
    "                Volumen (títulos) ejecutado por el mercado en cada bin del episodio.\n",
    "            - state_pos:\n",
    "                Número de step en el que nos encontramos.\n",
    "            - exec_vol:\n",
    "                Acumulado de títulos ejecutados por el algoritmo.\n",
    "            - action_hist:\n",
    "                Lista de acciones tomadas por el algoritmo en cada step.\n",
    "            - market_vwap_hist:\n",
    "                Lista de VWAP de mercado en cada step.\n",
    "            - reward_hist:\n",
    "                Lista de rewards obtenidas en cada step.\n",
    "            - price_hist:\n",
    "                Lista de precios ejecutados en cada step.\n",
    "            - vol_hist:\n",
    "                Lista de títulos ejecutados en cada step.                \n",
    "        \"\"\"\n",
    "        \n",
    "        # Fixed params\n",
    "        self.data = data\n",
    "        self.look_back = look_back\n",
    "        self.episode_bins = None\n",
    "        self.episode_full_len = None\n",
    "        self.vol_care = None\n",
    "        \n",
    "        self.actions_fn = {\n",
    "            0: self._do_nothing,\n",
    "            1: self._agg_action,\n",
    "        }\n",
    "        \n",
    "        self.n_actions = len(self.actions_fn)\n",
    "        self.n_features = self._detect_num_feat()\n",
    "\n",
    "        # Data variables\n",
    "        self.episode = None\n",
    "        self.episode_full = None\n",
    "        \n",
    "        # Env variables\n",
    "        self.episode_vwap = None\n",
    "        self.market_ep_vol = None\n",
    "        self.state_pos = 0\n",
    "        self.exec_vol = 0\n",
    "        self.actions_hist = []\n",
    "        self.algo_vwap_hist = []\n",
    "        self.market_vwap_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.price_hist = []\n",
    "        self.vol_hist = []\n",
    "        \n",
    "    def _detect_num_feat(self):\n",
    "        \"\"\"Detecta el número de variables del estado.\n",
    "        Función necesaria para adaptarse automaticamente a los\n",
    "        cambios en las variables de observation_builder.\n",
    "        \"\"\"\n",
    "        self._reset_env_episode_params()\n",
    "        self._generate_episode()\n",
    "        s = self.observation_builder()\n",
    "        return s.shape[0]\n",
    "        \n",
    "    def _reset_env_episode_params(self):\n",
    "        \"\"\"\n",
    "        Reset del episodio e inicialización de los parámetros.\n",
    "        Las variables internas vuelven a sus valores originales.\n",
    "        \"\"\"\n",
    "        self.episode_full_len = None\n",
    "        self.episode = None\n",
    "        self.episode_full = None\n",
    "        self.episode_vwap = None\n",
    "        self.market_ep_vol = None\n",
    "        self.state_pos = 0\n",
    "        self.algo_vwap = 0\n",
    "        self.exec_vol = 0\n",
    "        self.actions_hist = []\n",
    "        self.algo_vwap_hist = []\n",
    "        self.market_vwap_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.price_hist = []\n",
    "        self.vol_hist = []\n",
    "        \n",
    "    def _generate_episode_params(self):\n",
    "        \"\"\"Se determinan las características de la orden a ejecutar.\n",
    "        La órden queda definida por: \n",
    "         - episode_bins:\n",
    "             Obtención de un número entero aleatorio [400, 600] \n",
    "             con una distribución uniforme.\n",
    "         - vol_care:\n",
    "             Obtención del porcentaje de steps en el que hay que \n",
    "             ejecutar una órden para cubrir la órden care. \n",
    "             vol_care responde a un valor uniforme [0.075, 0.125]\n",
    "             multiplicado por el número self.episode_bins. \n",
    "             Lo convertimos a entero.\n",
    "        \"\"\"\n",
    "        # TODO: Int aleatorio entre 400 y 600 como un objeto numpy\n",
    "        self.episode_bins = np.random.randint(low=400, high=600)\n",
    "        # TODO: Float aleatorio entre 0.075 y 0.125\n",
    "        pct_bins = np.random.uniform(low=0.075, high=0.125)\n",
    "        # TODO: Int multiplicacion pct_bins y episode_bins\n",
    "        self.vol_care = int(pct_bins * self.episode_bins)\n",
    "        \n",
    "        self.episode_full_len = self.episode_bins + self.look_back\n",
    "        \n",
    "        assert self.episode_bins <= 600\n",
    "        assert self.episode_bins >= 400\n",
    "        assert self.vol_care <= int(self.episode_bins * 0.125)\n",
    "        assert self.vol_care >= int(self.episode_bins * 0.075)\n",
    "        assert isinstance(self.vol_care, int)\n",
    "        \n",
    "    def _generate_episode(self):\n",
    "        \"\"\"Obtenemos el día y hora en el que comienza el episodio.\n",
    "        \"\"\"\n",
    "        self._generate_episode_params()\n",
    "        \n",
    "        lenght_episode = 0\n",
    "        while lenght_episode != self.episode_full_len:\n",
    "            # TODO: Selección de un dia entre los posibles.\n",
    "            # Clue: Usa np.random.choice y los dias data.keys\n",
    "            selected_day = np.random.choice(\n",
    "                    list(self.data.keys()\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # TODO: Extrae selected_day de data\n",
    "            data_day = self.data[selected_day]\n",
    "            \n",
    "            # TODO: selecciona una hora de inicio aleatoria\n",
    "            init_time = np.random.choice(data_day.index)\n",
    "            \n",
    "            hour_pos = data_day.index.get_loc(init_time)\n",
    "            initial_position = hour_pos - self.look_back\n",
    "            final_position = hour_pos + self.episode_bins\n",
    "            \n",
    "            if initial_position < 0:\n",
    "                continue\n",
    "            else:\n",
    "                # TODO: Filtra data_day entre por initial_position y final_position\n",
    "                self.episode_full = data_day.iloc[initial_position:final_position, :]\n",
    "                \n",
    "                # TODO: Filtra data_day entre por hour_pos y final_position\n",
    "                self.episode = data_day.iloc[hour_pos:final_position, :]\n",
    "                \n",
    "                lenght_episode = self.episode_full.shape[0]\n",
    "        \n",
    "            \n",
    "    def reset(self) -> np.array:\n",
    "        \"\"\"Reinicialización del episodio junto con los parámetros.\n",
    "        Devuelve la primera observación del nuevo episodio.\n",
    "        \"\"\"  \n",
    "        self._reset_env_episode_params()     \n",
    "        self._generate_episode()\n",
    "        self._compute_episode_market_feat()\n",
    "        \n",
    "        return self.observation_builder()\n",
    "    \n",
    "    def observation_builder(self) -> np.array:\n",
    "        \"\"\" Función para la construcción de las observaciones del estado.\n",
    "            ------------------------------------------------------------\n",
    "            Default:\n",
    "                - Primera característica es tiempo restante en porcentaje.\n",
    "                - Seguna característica es el volumen restante en porcentaje.\n",
    "        \"\"\"\n",
    "        # TODO: Construye el vector con las dos características de la descripción\n",
    "        # Clue: Utiliza episode_bins, state_pos, exec_vol ,vol_care\n",
    "        time_left = (self.episode_bins - self.state_pos) / self.episode_bins\n",
    "        vol_left = 1 - (self.exec_vol / self.vol_care)\n",
    "        obs  = np.array([time_left, vol_left])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _compute_episode_market_feat(self) -> Tuple[float, float]:\n",
    "        \"\"\"Cáculo de los valores VWAP y Market Vol del episodio.\n",
    "        Como no tenemos las ejecuciones de mercado, asumimos que el \n",
    "        precio es el mid price de cada step.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula el mid price utilizando ask1 y bid1 de episode\n",
    "        # Opcional: Utiliza un precio más realista para el mkt VWAP\n",
    "        mid = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        # TODO: Calcula market_ep_vol\n",
    "        self.market_ep_vol = self.episode.cumvol.diff()\n",
    "        self.market_ep_vol[0] = 0\n",
    "        # TODO: calcula el volumen acumulado del mercado en todo el episodio\n",
    "        cum_vol = self.market_ep_vol.sum()\n",
    "        # TODO: calcula el episode_vwap\n",
    "        self.episode_vwap = (mid[:-1] * self.market_ep_vol[1:]).sum() / cum_vol\n",
    "        \n",
    "        return self.episode_vwap, self.market_ep_vol\n",
    "    \n",
    "    def _compute_algo_vwap(self) -> float:\n",
    "        \"\"\"Cálculo del VWAP del algoritmo hasta el step actual.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula el algo_vwap\n",
    "        # Clue: utiliza price_hist, vol_hist\n",
    "        p_arr = np.array(self.price_hist)\n",
    "        v_arr = np.array(self.vol_hist)\n",
    "        algo_vwap = np.sum(p_arr * v_arr) / np.sum(v_arr)\n",
    "        return algo_vwap\n",
    "    \n",
    "    def _compute_reward(self, price: float, vol: float) -> float:\n",
    "        \"\"\"Función de diseño de los rewards y penalizaciónes que \n",
    "        recibe el algoritmo al tomar las acciones.\n",
    "        --------------------------------------------------------\n",
    "        Default:\n",
    "            - El reward es el ratio de la diferencia entre el episode_vwap y\n",
    "              el precio de la acción tomada, dividido entre episode_vwap.\n",
    "        \"\"\"\n",
    "        # TODO: Establece y devuelve un reward cuando vol == 0\n",
    "        if vol == 0:\n",
    "            reward = 0\n",
    "            return reward\n",
    "        # TODO: Calcula y devuelve el reward cuando vol > 0\n",
    "        # Clue: Utiliza episode_vwap y price para la reward por defecto\n",
    "        # Opcional: Utiliza el self y elimina los parámetros de la función\n",
    "        reward = (self.episode_vwap - price) / self.episode_vwap\n",
    "        return reward\n",
    "    \n",
    "    def _compute_stop_conditions(self) -> Tuple[bool, bool]:\n",
    "        \"\"\"Define las condiciones de parada del episodio\n",
    "        Return:\n",
    "            Tiempo agotado, orden completada\n",
    "        \"\"\"\n",
    "        # TODO: Calcula las variables de parada y devuélvelas en el orden apropiado\n",
    "        is_bins_complete = self.state_pos == self.episode_bins\n",
    "        is_ord_complete = self.exec_vol == self.vol_care\n",
    "        return is_bins_complete, is_ord_complete\n",
    "    \n",
    "    def _compute_done_reward(self) -> float:\n",
    "        # TODO: Free style\n",
    "        _, is_ord_complete = self._compute_stop_conditions()\n",
    "        rwd_factor = not is_ord_complete\n",
    "        done_reward = -1 * rwd_factor\n",
    "        return done_reward\n",
    "    \n",
    "    def _agg_action(self) -> float:\n",
    "        \"\"\"Acción agresiva de compra de un título a precio de episode['ask1'].\n",
    "        Devolvemos el reward asociado a esa acción.\n",
    "        \"\"\"\n",
    "        # TODO: obtén el precio de la accion agresiva (ask1) en el state_pos\n",
    "        price = self.episode[\"ask1\"].values[self.state_pos]\n",
    "        # TODO: guarda price en price_hist, añade 1 a exec_vol y añade 1 a vol_hist\n",
    "        self.price_hist.append(price)\n",
    "        exec_vol = 1\n",
    "        self.exec_vol += exec_vol\n",
    "        self.vol_hist.append(exec_vol)\n",
    "        \n",
    "        # TODO: utiliza la función apropiada para calcula el algo_vwap\n",
    "        algo_vwap = self._compute_algo_vwap()\n",
    "        # guarda el algo_vwap en algo_vwap_hist\n",
    "        self.algo_vwap_hist.append(algo_vwap)\n",
    "        # TODO: calcula el reward utilizando la función apropiada\n",
    "        reward = self._compute_reward(price, exec_vol)\n",
    "        return reward\n",
    "\n",
    "    def _do_nothing(self) -> float:\n",
    "        \"\"\"No hacer nada y devolvemos el reward asociado a la acción\n",
    "        \"\"\"\n",
    "        # TODO: Repite el proceso de _agg_action\n",
    "        # Clue: Precio y volumen ejecutado = 0\n",
    "        price = 0\n",
    "        exec_vol = 0\n",
    "        self.price_hist.append(price)\n",
    "        self.vol_hist.append(exec_vol)\n",
    "        algo_vwap = self.algo_vwap_hist[-1]\n",
    "        self.algo_vwap_hist.append(algo_vwap)\n",
    "        reward = self._compute_reward(price, exec_vol)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _compute_market_vwap(self) -> float:\n",
    "        \"\"\"Cálculo del VWAP del mercado hasta el step actual.\n",
    "        \"\"\"\n",
    "        # TODO: Establece un para el vol ejecutado por el mkt en cada step\n",
    "        # Clue: puedes fijarte en _compute_episode_market_feat\n",
    "        mid_p = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        mkt_p = (mid_p + mid_p.shift(-1).ffill()) / 2\n",
    "        # Calcula todos los vwap del mkt hasta el step actual incluido\n",
    "        v = self.episode[\"cumvol\"].diff().shift(-1)\n",
    "        p_arr = mkt_p.values[:self.state_pos + 1]\n",
    "        v_arr = v.values[:self.state_pos + 1]\n",
    "        sum_vol = np.sum(v_arr)\n",
    "        # Si el mkt vol hasta el step == 0, devuelve el último precio hasta el step\n",
    "        if sum_vol == 0:\n",
    "            return p_arr[-1]\n",
    "        # Calcula y devuelve el vwap acumulado hasta el step\n",
    "        market_vwap = np.sum(p_arr * v_arr) / sum_vol\n",
    "        return market_vwap\n",
    "    \n",
    "    def _compute_done(self) -> bool:\n",
    "        \"\"\" Reglas de finalización del episodio.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula las condiciones de parada utilizando la función adecuada\n",
    "        conditions = self._compute_stop_conditions()\n",
    "        is_bins_complete = conditions[0]\n",
    "        is_ord_complete = conditions[1]\n",
    "        # TODO: Devuelve done == True si se cumplen cualquiera de las condiciones\n",
    "        done = is_bins_complete or is_ord_complete\n",
    "        return done\n",
    "\n",
    "    def step(self, action) -> Tuple[np.array, float, bool, dict]:\n",
    "        \"\"\" Evalua la accián, calcula la recompensa, devuelve el \n",
    "        nuevo estado y si el episodio ha terminado.\n",
    "        \"\"\"\n",
    "        \n",
    "        market_vwap = self._compute_market_vwap()\n",
    "        act_fn = self.actions_fn.get(a)\n",
    "        if act_fn is None:\n",
    "            raise ValueError(\n",
    "                f\"Invalid action {a}. Valid actions {self.actions_fn.keys()}\"\n",
    "            )\n",
    "        \n",
    "        reward = act_fn()\n",
    "\n",
    "        self.market_vwap_hist.append(market_vwap)\n",
    "        self.reward_hist.append(reward)\n",
    "        \n",
    "        self.state_pos += 1\n",
    "        \n",
    "        done = self._compute_done()\n",
    "        \n",
    "        if done:\n",
    "            reward += self._compute_done_reward()\n",
    "            return None, reward, done, {}\n",
    "        \n",
    "        observation = self.observation_builder()\n",
    "        \n",
    "        return np.array(observation), reward, done, {}\n",
    "    \n",
    "    def action_sample(self):\n",
    "        \"\"\"\n",
    "        Devuelve una acción aleatoria. El valor ha de corresponder \n",
    "        con las keys de actions_fn.\n",
    "        \"\"\"\n",
    "        # TODO: Toma una acción aleatoria\n",
    "        # Opcional: ¿Qué distribución de prob es mejor para la exploración?\n",
    "        p = self.vol_care / self.episode.shape[0]\n",
    "        action = np.random.choice([0, 1], p=[1-p, p])\n",
    "        return action\n",
    "\n",
    "    def stats_df(self):\n",
    "        \"\"\"Información para el gráfico de resultados de la ejecución\n",
    "        \"\"\"\n",
    "        \n",
    "        my_df = pd.DataFrame(\n",
    "            {\"vwap\": self.algo_vwap_hist, \"vol\": self.vol_hist},\n",
    "            index=list(self.episode.index)[:len(self.algo_vwap_hist)]\n",
    "        )\n",
    "        my_df = my_df.reindex(self.episode.index)\n",
    "        my_df[\"vol\"] = my_df[\"vol\"].fillna(0)\n",
    "        my_df[\"vwap\"] = my_df[\"vwap\"].ffill()\n",
    "            \n",
    "        \n",
    "        p = self.episode[\"ask1\"]\n",
    "        v = self.episode[\"cumvol\"].diff().shift(-1)\n",
    "        last_v = self.episode_full[\"cumvol\"].diff()[-1]\n",
    "        v.iloc[-1] = last_v\n",
    "        market_vwap = (p * v).cumsum() / v.cumsum()\n",
    "        market_df = pd.DataFrame(\n",
    "            {\"vwap\": market_vwap, \"vol\": v},\n",
    "            index=v.index\n",
    "        )\n",
    "        \n",
    "        mpx = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        \n",
    "        return my_df, market_df, mpx\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-washington",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h1>TWAP</h1></center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "El <b><i>time weighted average price</i></b> (TWAP) es un algoritmo tradicional de negociación basado en el precio medio ponderado utilizado para la ejecución de órdenes más grandes sin un impacto excesivo en el precio de mercado. Básicamente, se trocea la orden de manera que ejecute las ordenes en un tiempo equidistante durante la sesión en la que se pide la ejecución de la orden CARE. Puede ser fácil adivinar el patrón de negociación de la estrategia en ejecución si sus órdenes no se modifican de una manera especial, por lo que los parámetros se pueden ajustar para hacer que la estrategia sea más difícil de rastrear. Las soluciones más comunes son la distribución aleatoria del tamaño de los pedidos y / o el tiempo de demora entre ellos. Es posible limitar la cantidad para que no exceda un porcentaje definido del volumen de participación, para minimizar el impacto de las estrategias en el mercado.\n",
    "</p>  \n",
    "<p style=\"width:80%\">\n",
    "Dada las caracteristicas del taller, se pide a los participantes implementar un TWAP sin la necesidad de realizar ninguna modificación. El agente TWAP ha de trocear y ejecutar las órdenes hijas de la orden CARE equidistantes en el tiempo. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TWAP(DDQNAgent):\n",
    "    def act(self, s):\n",
    "        # TODO: Configura un TWAP determinista utilizando s\n",
    "        if s[1] >= s[0]:\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-pound",
   "metadata": {},
   "source": [
    "------------------\n",
    "<center><h1>Parametrización del Problema</h1></center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "Una vez definido el entorno que vamos a utilizar como simulador de mercado necesitamos definir el agente que va a encargarse de aprender a ejecutar las órdenes CARE. El agente es el <b><i>cerebro</i></b> del sistema y puede tener o no tener inteligencia artificial implementada. Por ejemplo, la clase TWAP anteriormente implementada depende unicamente de una regla determinista. Por otro lado, los algoritmos vistos durante el bloque de Aprendizaje por Refuerzo dependeran de la construcción de un modelo de aprendizaje y por tanto requerirán de la parametrización de estos. Los parámetros e hiperparámetros del modelo dependerán de la aproximación y modelo que elijamos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-diversity",
   "metadata": {},
   "source": [
    "---\n",
    "<center >\n",
    "<p style=\"width:80%\">\n",
    "    <b>Epsilon</b>: Parámetro de exploración para los algoritmos basados en funciones valor (por ejemplo, los deep Q-learning). Puede tomar valores entre [0,1].\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Min Epsilon</b>: Valor mínimo del epsilon por si se quier dejar aleatoriedad durante procesos de entrenamiento muy largos en los que el <b>epsilon</b> decaería hasta cero.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Espilon_decay</b>: Valor numérico mediante el cual se reduce el valor de epsilon cada determinado número de episodios.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Gamma</b>: Término de descuento para el calculo de los retornos acumulados con descuento. Dado que nuestro MDP se define con tareas episódicas gamma podrá tomar valores en el intervalo [0,1].\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Alpha</b>: Step-size, término indicando el tamaño de las actualizaciones durante el aprendizaje. Dado que nuestro problema es no estacionario (el sistema evoluciona en el tiempo) el alpha será un valor estático o adaptativo pero nunca un valor no descendente a cero en el tiempo.\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Buffer Size</b>: Tamaño de la memoria encargada de obtener un conjunto de datos que aproximadamente sea independiente e identicamente distribuido (i.i.d).\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "---\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "<b>Nota</b>: Los parametros que definen la arquitectura de la red dependen de lo flexible y compleja que se contruyan en los agentes. Los agentes ofrecidos para el taller mantienen la filosofia del mismo y se mantendrán sencillos.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Batch size</b>: Tamaño de los paquetes de datos que se envian al modelo durante su aprendizaje.\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Hidden_neurons</b>: Número de neuronas en las capatas ocultas de los modelos.\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "---\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Nepisodes</b>: Número de sessiones de trading (episodios) que se vana realizar para el entrenamiento del modelo.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>N_log</b>: Valor numérico indicando cada cuantos episodios se realiza un log de la situación actual del aprendizaje.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Learn_after</b>: Valor numérico indicando cada cuantos steps se realiza un aprendizaje por parte de las redes.\n",
    "</p> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Agent Params\n",
    "\"\"\"\n",
    "# TODO: Selecciona un epsilon inicial para el entrenamiento\n",
    "epsilon = 1\n",
    "# TODO: Selecciona un min_epsilon para el entrenamiento\n",
    "min_epsilon = 0.05\n",
    "# TODO: Selecciona un gamma para el aprendizaje\n",
    "gamma = 1\n",
    "# TODO: Selecciona un alpha para el aprendizaje\n",
    "alpha = 0.0001\n",
    "# TODO: Selecciona un buffer_size para el aprendizaje\n",
    "buffer_size = 80000\n",
    "# TODO: Selecciona un batch_size para el aprendizaje\n",
    "batch_size = 256\n",
    "# TODO: Selecciona el número de nueronas para el modelo\n",
    "hidden_neurons = 240\n",
    "\n",
    "\"\"\"\n",
    "    Training Params\n",
    "\"\"\"\n",
    "# TODO: Selecciona el número de episodios\n",
    "nepisodes = 1000\n",
    "n_log = 25\n",
    "#TODO: Determina el epsilon_decay para el proceso de entrenamiento\n",
    "epsilon_decay = (epsilon - min_epsilon) / (nepisodes * 0.95)\n",
    "learn_after = batch_size\n",
    "\n",
    "env = BestExecutionEnv(data, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-holocaust",
   "metadata": {},
   "source": [
    "<center><h3>Inicialización del agente</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(\n",
    "    env, gamma=gamma, epsilon=epsilon, alpha=alpha,\n",
    "    batch_size=batch_size, buffer_size=buffer_size,\n",
    "    hidden_neurons=hidden_neurons, trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-request",
   "metadata": {},
   "source": [
    "<center><h1>Recolección de los Episodios para el Buffer</h1></center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "Antes de empezar el entrenamiento, para que el modelo tenga experiencias de las que aprender y poder tomar acciones greedy con cierto criterio ha de tener muestras para entrenar. Para ello vamos a llenar el buffer con experiencias siguendo una política estocástica con el objetivo de explorar entorno inicialmente.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto eps es 1 -> actuando random\n",
    "s = env.reset()\n",
    "for exps in range(buffer_size):  \n",
    "    a = agent.act(s)\n",
    "    s1, r, done, _ = env.step(a)\n",
    "    agent.experience(s, a, r, s1, done)\n",
    "    s = s1\n",
    "\n",
    "    if not exps % 10000:\n",
    "        print(f'buffer exps: {exps}')\n",
    "    if done:\n",
    "        s = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-brown",
   "metadata": {},
   "source": [
    "#### Train Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_trainable(True)\n",
    "learn_counter = 0\n",
    "history_steps = []\n",
    "history_rewards = []\n",
    "history_disc_rewards = []\n",
    "history_losses = []\n",
    "\n",
    "list_df = []\n",
    "list_market_df = []\n",
    "list_mpx = []\n",
    "\n",
    "for episode in range(nepisodes):\n",
    "    s = env.reset()\n",
    "    step = 0\n",
    "    cum_reward = 0\n",
    "    dis_cum_reward = 0\n",
    "    episode_losses = []\n",
    "    while True:\n",
    "        a = agent.act(s)\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        agent.experience(s, a, r, s1, done)\n",
    "        learn_counter += 1\n",
    "        cum_reward += r\n",
    "        dis_cum_reward += agent.gamma ** step * r\n",
    "        s = s1\n",
    "        step += 1\n",
    "        if not learn_counter % learn_after:\n",
    "            mse = agent.learn()\n",
    "        if done:\n",
    "            agent.epsilon = max([agent.epsilon - epsilon_decay, min_epsilon])\n",
    "            history_rewards.append(cum_reward)\n",
    "            history_disc_rewards.append(dis_cum_reward)\n",
    "            history_losses.append(mse)\n",
    "            history_steps.append(step)\n",
    "            \n",
    "            res = env.stats_df()\n",
    "            list_df.append(res[0])\n",
    "            list_market_df.append(res[1])\n",
    "            list_mpx.append(res[2])\n",
    "            \n",
    "            if not episode % n_log:\n",
    "                mse = agent.learn()\n",
    "                print(\n",
    "                    f'Episode: {episode}, '\n",
    "                    f'steps: {np.round(np.mean(history_steps[-n_log:]), 2)}, '\n",
    "                    f'rew: {np.round(np.mean(history_rewards[-n_log:]), 2)}, '\n",
    "                    f'mse: {np.round(mse)}, '\n",
    "                    f'eps: {np.round(agent.epsilon, 2)}'\n",
    "                )\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-visibility",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_trainable(False)\n",
    "cum_reward = 0\n",
    "step = 0\n",
    "env = BestExecutionEnv(df[\"test\"], 60)\n",
    "s = env.reset()\n",
    "a = 1\n",
    "s, r, done, _ = env.step(a)\n",
    "step += 1\n",
    "cum_reward += agent.gamma ** step * r\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    s, r, done, _ = env.step(a)\n",
    "    step += 1\n",
    "    cum_reward += agent.gamma ** step * r\n",
    "    if done:\n",
    "        break\n",
    "plot_results(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = env.stats_df()\n",
    "grupo_ = pd.cut(b.index, 400)\n",
    "b[\"grupo\"] = grupo_\n",
    "b = b.loc[:, [\"vwap\", \"grupo\"]]\n",
    "b = b.groupby([\"grupo\"]).mean().bfill()\n",
    "a[\"grupo\"] = grupo_\n",
    "a = a.loc[:, [\"vol\", \"grupo\"]]\n",
    "a = a.groupby([\"grupo\"]).max().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(b.values, color='red')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(a.values, color='blue')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.exec_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.vol_care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_rewards).rolling(20).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_markets = []\n",
    "for market_df in list_market_df:\n",
    "    grupo = pd.cut(market_df.index, 400)\n",
    "    market_df[\"grupo\"] = grupo\n",
    "    market_df = market_df.loc[:, [\"vwap\", \"grupo\"]]\n",
    "    market_df = market_df.groupby([\"grupo\"]).mean().bfill()\n",
    "    res_markets.append(market_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fin = res_markets[0].values\n",
    "for i in res_markets[1:]:\n",
    "    df_fin += i.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-springer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
