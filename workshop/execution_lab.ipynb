{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "filled-clone",
   "metadata": {},
   "source": [
    "<center><h1>Hands-On Reinforcement Learning Applied to Trade Execution Algorithms</h1></center>\n",
    "<center>\n",
    "Autor: <cite><a href=\"https://www.linkedin.com/in/aoteog/\">Oteo García, Alberto</a></cite>\n",
    "</center>\n",
    "<center>\n",
    "Autor: <cite><a href=\"https://www.linkedin.com/in/jesus-sanz/\">Sanz del Real, Jesús</a></cite>\n",
    "</center>\n",
    "\n",
    "----------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-ozone",
   "metadata": {},
   "source": [
    "<center><h1>Resumen</h1></center>\n",
    "\n",
    "<center>\n",
    "\n",
    "<p style=\"width:80%\">\n",
    "    En este notebook se presenta el taller de aprendizaje por refuerzo aplicado a un caso real para finanzas. \n",
    "    El objetivo es plantear, desde cero, la resolución del problema de <b>algoritmos de ejecución</b> a través del uso\n",
    "    de técnicas de aprendizaje por refuerzo. Para ello, durante el taller se ha de completar el entorno básico <b>BestExecutionEnv</b> donde los asistentes desarrollarán las partes claves del\n",
    "    código para obtener un simulador de mercado que, aunque relativamente naive, será funcional. Posteriormente, los participantes tendrán que aplicar algoritmos clásicos y basados en inteligencia artificial (IA) para la\n",
    "    resolución del problema. \n",
    "</p>\n",
    "<p style=\"width:80%\">\n",
    "    En términos sencillos, el problema de <b>algoritmos de ejecución</b> trata sobre la ejecución de órdenes con un tamaño lo suficientemente grande como para que su ejecución suponga un <b><i>'problema'</i></b>, como se ha comentado durante la explicación del caso práctico. La resolución de la ejecución de estas órdenes (órdenes CARE) se puede realizar utilizando diferentes aproximaciónes. En el caso de este taller, queremos optimizar el <b><i>timing</i></b> de nuestras órdenes para minimizar el impacto de mercado. Es decir, buscaremos trocear la orden CARE para comprar (vender) a los menores (mayores) precios de la sesión en la que se ejecuta la orden CARE. Como baseline para saber si nuestra ejecución es buena, o mala, utilizaremos el baseline habitual en el sector: el VWAP slippage.\n",
    "</p> \n",
    "<p style=\"width:80%\">\n",
    "El VWAP slipagge es la desviación, en puntos básicos, del VWAP del mercado en comparación con el VWAP de nuestro algoritmo. Siendo el VWAP el precio medio ponderado por el volumen:\n",
    "</p>   \n",
    "    \n",
    "$$\\frac{\\sum_{i=1}^T p_i\\cdot v_i}{\\sum_{i=1}^Tv_i}$$\n",
    "\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-germany",
   "metadata": {},
   "source": [
    "<center><h1>Datos</h1></center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "Los datos para este taller se han extraido de los activos Repsol y Santander (<i>rep_data.pickle</i> y <i>san_data.pickle</i>). Entre la información extraida contamos con la primera posicion del libro de ordenes tanto el bid como del ask, así como el volumen acumulado, todo ello realizando una agrupación por 5 segundos. Las sesiones extraidas del libro de órdenes están comprendidas entre las 13:00h y las 17:25h. El conjunto de datos esta partido en 3 subconjuntos, para los siguientes días bursátiles:\n",
    "</p>\n",
    "    \n",
    "</center> \n",
    "<center>\n",
    "    <p style=\"width:35%\"><b>Train</b>: Primeros 60 días hábiles de 2018</p>\n",
    "    <p style=\"width:35%\"><b>Validación</b>: Primeros 30 días hábiles de 2019</p>\n",
    "    <p style=\"width:35%\"><b>Test</b>: Intervalo de los días hábiles del 31 al 60 de 2019 (30 días)</p>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "<b>Adicionalmente</b>, para aquellos que quieran aumentar el grado de datalle, se han añadido otros dos ficheros con información más completa del libro de órdenes y de las ejecuciónes de mercado. El fichero <i>orderbook.pkl</i> contiene las posiciones (precios y volúmenes) de hasta la tercera posición del libro de órdenes, agrupados por 5 segundos; el fichero <i>executions.pkl</i> todas las ejecuciones del mercado. En ambos casos, se trata de información para la compañía Repsol, entre las 13:00h y 17:25h de la sesión. El conjunto de datos esta partido en 3 subconjuntos, para los siguientes días bursátiles:\n",
    "</p>\n",
    "    \n",
    "</center> \n",
    "<center>\n",
    "    <p style=\"width:35%\"><b>Train</b>: Primeros 40 días hábiles de 2019</p>\n",
    "    <p style=\"width:35%\"><b>Validación</b>: Intervalo del 41 al 60 2019 (20 días)</p>\n",
    "    <p style=\"width:35%\"><b>Test</b>: Intervalo del 61 al 80 2019 (20 días)</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import pickle\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from agents.dqn import DDQNAgent\n",
    "from report.report import plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos disponibles: san_data.pickle y rep_data.pickle\n",
    "with open(\"../data/san_data.pickle\", \"rb\") as f:\n",
    "        dict_ = pickle.load(f)\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-thought",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = dict_[\"train\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-employment",
   "metadata": {},
   "source": [
    "---\n",
    "# Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestExecutionEnv:\n",
    "\n",
    "    def __init__(self, data, look_back=60):\n",
    "        \"\"\"Inicialización de la clase del entorno que simula\n",
    "        el libro de ordenes.\n",
    "        ----------------------------------------------------\n",
    "        Input:\n",
    "            - data: \n",
    "                Dataframe con los datos previamente\n",
    "                agrupados del libro de órdenes.\n",
    "            \n",
    "            - look_back: \n",
    "                Ventana para la generación de features\n",
    "                roladas en el instante t=0 del episodio.\n",
    "                Esta ventana representa el rango máximo para\n",
    "                la construcción de features.\n",
    "        ----------------------------------------------------\n",
    "        Variables Internas:\n",
    "            - episode_bins:\n",
    "                Número de bines (steps) del episodio.\n",
    "            - episode_full_len:\n",
    "                Es igual a look_back + episode_bins.\n",
    "            - vol_care:\n",
    "                Volumen total (en títulos) de la orden care.\n",
    "            - actions_fn:\n",
    "                Diccionario con las posibles acciones del agente.\n",
    "                Las claves acceden a la función que evalúa  la acción\n",
    "                tomada por el agente.\n",
    "            - n_actions:\n",
    "                Número de acciones posibles.\n",
    "            - n_features:\n",
    "                Número de características de los estados.\n",
    "            - episode:\n",
    "                Dataframe que contiene los steps y estados del episodio.\n",
    "            - episode_full:\n",
    "                Es el episode añadiendo el look_back antes del comienzo \n",
    "                del episodio.\n",
    "            - episode_vwap:\n",
    "                VWAP de mercado al final del episodio.\n",
    "            - market_ep_vol:\n",
    "                Volumen (títulos) ejecutado por el mercado en cada bin del episodio.\n",
    "            - state_pos:\n",
    "                Número de step en el que nos encontramos.\n",
    "            - exec_vol:\n",
    "                Acumulado de títulos ejecutados por el algoritmo.\n",
    "            - action_hist:\n",
    "                Lista de acciones tomadas por el algoritmo en cada step.\n",
    "            - market_vwap_hist:\n",
    "                Lista de VWAP de mercado en cada step.\n",
    "            - reward_hist:\n",
    "                Lista de rewards obtenidas en cada step.\n",
    "            - price_hist:\n",
    "                Lista de precios ejecutados en cada step.\n",
    "            - vol_hist:\n",
    "                Lista de títulos ejecutados en cada step.                \n",
    "        \"\"\"\n",
    "        \n",
    "        # Fixed params\n",
    "        self.data = data\n",
    "        self.look_back = look_back\n",
    "        self.episode_bins = None\n",
    "        self.episode_full_len = None\n",
    "        self.vol_care = None\n",
    "        \n",
    "        self.actions_fn = {\n",
    "            0: self._do_nothing,\n",
    "            1: self._agg_action,\n",
    "        }\n",
    "        \n",
    "        self.n_actions = len(self.actions_fn)\n",
    "        self.n_features = self._detect_num_feat()\n",
    "\n",
    "        # Data variables\n",
    "        self.episode = None\n",
    "        self.episode_full = None\n",
    "        \n",
    "        # Env variables\n",
    "        self.episode_vwap = None\n",
    "        self.market_ep_vol = None\n",
    "        self.state_pos = 0\n",
    "        self.exec_vol = 0\n",
    "        self.actions_hist = []\n",
    "        self.algo_vwap_hist = []\n",
    "        self.market_vwap_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.price_hist = []\n",
    "        self.vol_hist = []\n",
    "        \n",
    "    def _detect_num_feat(self):\n",
    "        \"\"\"Detecta el número de variables del estado.\n",
    "        Función necesaria para adaptarse automaticamente a los\n",
    "        cambios en las variables de observation_builder.\n",
    "        \"\"\"\n",
    "        self._reset_env_episode_params()\n",
    "        self._generate_episode()\n",
    "        s = self.observation_builder()\n",
    "        return s.shape[0]\n",
    "        \n",
    "    def _reset_env_episode_params(self):\n",
    "        \"\"\"\n",
    "        Reset del episodio e inicialización de los parámetros.\n",
    "        Las variables internas vuelven a sus valores originales.\n",
    "        \"\"\"\n",
    "        self.episode_full_len = None\n",
    "        self.episode = None\n",
    "        self.episode_full = None\n",
    "        self.episode_vwap = None\n",
    "        self.market_ep_vol = None\n",
    "        self.state_pos = 0\n",
    "        self.algo_vwap = 0\n",
    "        self.exec_vol = 0\n",
    "        self.actions_hist = []\n",
    "        self.algo_vwap_hist = []\n",
    "        self.market_vwap_hist = []\n",
    "        self.reward_hist = []\n",
    "        self.price_hist = []\n",
    "        self.vol_hist = []\n",
    "        \n",
    "    def _generate_episode_params(self):\n",
    "        \"\"\"(1) Se determinan las características de la orden a ejecutar.\n",
    "        La órden queda definida por: \n",
    "         - episode_bins:\n",
    "             Obtención de un número entero aleatorio [400, 600] \n",
    "             con una distribución uniforme.\n",
    "         - vol_care:\n",
    "             Obtención del porcentaje de steps en el que hay que \n",
    "             ejecutar una órden para cubrir la órden care. \n",
    "             vol_care responde a un valor uniforme [0.075, 0.125]\n",
    "             multiplicado por el número self.episode_bins. \n",
    "             Lo convertimos a entero.\n",
    "        \"\"\"\n",
    "        # TODO: Int aleatorio entre 400 y 600 como un objeto numpy\n",
    "        self.episode_bins = \"-----\"\n",
    "        # TODO: Float aleatorio entre 0.075 y 0.125\n",
    "        pct_bins = \"-----\"\n",
    "        # TODO: Int multiplicacion pct_bins y episode_bins\n",
    "        self.vol_care = \"-----\"\n",
    "        \n",
    "        self.episode_full_len = self.episode_bins + self.look_back\n",
    "        \n",
    "        assert self.episode_bins <= 600\n",
    "        assert self.episode_bins >= 400\n",
    "        assert self.vol_care <= int(self.episode_bins * 0.125)\n",
    "        assert self.vol_care >= int(self.episode_bins * 0.075)\n",
    "        assert isinstance(self.vol_care, int)\n",
    "        \n",
    "    def _generate_episode(self):\n",
    "        \"\"\"(2) Obtenemos el día y hora en el que comienza el episodio.\n",
    "        \"\"\"\n",
    "        self._generate_episode_params()\n",
    "        \n",
    "        lenght_episode = 0\n",
    "        while lenght_episode != self.episode_full_len:\n",
    "            # TODO: Selección de un dia entre los posibles.\n",
    "            # Clue: Usa np.random.choice y los dias data.keys\n",
    "            selected_day = \"-----\"\n",
    "            \n",
    "            # TODO: Extrae selected_day de data\n",
    "            data_day = \"-----\"\n",
    "            \n",
    "            # TODO: selecciona una hora de inicio aleatoria\n",
    "            init_time = \"-----\"\n",
    "            \n",
    "            hour_pos = data_day.index.get_loc(init_time)\n",
    "            initial_position = hour_pos - self.look_back\n",
    "            final_position = hour_pos + self.episode_bins\n",
    "            \n",
    "            if initial_position < 0:\n",
    "                continue\n",
    "            else:\n",
    "                # TODO: Filtra data_day entre por initial_position y final_position\n",
    "                self.episode_full = \"-----\"\n",
    "                \n",
    "                # TODO: Filtra data_day entre por hour_pos y final_position\n",
    "                self.episode = \"-----\"\n",
    "                \n",
    "                lenght_episode = self.episode_full.shape[0]\n",
    "        \n",
    "            \n",
    "    def reset(self) -> np.array:\n",
    "        \"\"\"Reinicialización del episodio junto con los parámetros.\n",
    "        Devuelve la primera observación del nuevo episodio.\n",
    "        \"\"\"  \n",
    "        self._reset_env_episode_params()     \n",
    "        self._generate_episode()\n",
    "        self._compute_episode_market_feat()\n",
    "        \n",
    "        return self.observation_builder()\n",
    "    \n",
    "    def observation_builder(self) -> np.array:\n",
    "        \"\"\"(3) Función para la construcción de las observaciones del estado.\n",
    "            ------------------------------------------------------------\n",
    "            Default:\n",
    "                - Primera característica es tiempo restante en porcentaje.\n",
    "                - Seguna característica es el volumen restante en porcentaje.\n",
    "        \"\"\"\n",
    "        # TODO: Construye el vector con las dos características de la descripción\n",
    "        # Clue: Utiliza episode_bins, state_pos, exec_vol ,vol_care\n",
    "        time_left = \"-----\"\n",
    "        vol_left = \"-----\"\n",
    "        obs  = np.array([time_left, vol_left])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _compute_episode_market_feat(self) -> Tuple[float, float]:\n",
    "        \"\"\"(4) Cálculo de los valores VWAP y Market Vol del episodio.\n",
    "        Como no tenemos las ejecuciones de mercado, asumimos que el \n",
    "        precio es el mid price de cada step.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula el mid price utilizando ask1 y bid1 de episode\n",
    "        # Opcional: Utiliza un precio más realista para el mkt VWAP\n",
    "        mid = \"-----\"\n",
    "        # TODO: Calcula market_ep_vol\n",
    "        self.market_ep_vol = \"-----\"\n",
    "        self.market_ep_vol[0] = 0\n",
    "        # TODO: calcula el volumen acumulado del mercado en todo el episodio\n",
    "        cum_vol = \"-----\"\n",
    "        # TODO: calcula el episode_vwap\n",
    "        self.episode_vwap = \"-----\"\n",
    "        \n",
    "        return self.episode_vwap, self.market_ep_vol\n",
    "    \n",
    "    def _compute_algo_vwap(self) -> float:\n",
    "        \"\"\"(8) Cálculo del VWAP del algoritmo hasta el step actual.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula el algo_vwap\n",
    "        # Clue: utiliza price_hist, vol_hist\n",
    "        p_arr = \"-----\"\n",
    "        v_arr = \"-----\"\n",
    "        algo_vwap = \"-----\"\n",
    "        return algo_vwap\n",
    "    \n",
    "    def _compute_reward(self, price: float, vol: float) -> float:\n",
    "        \"\"\"(6)Función de diseño de los rewards y penalizaciónes que \n",
    "        recibe el algoritmo al tomar las acciones.\n",
    "        --------------------------------------------------------\n",
    "        Default:\n",
    "            - El reward es el ratio de la diferencia entre el episode_vwap y\n",
    "              el precio de la acción tomada, dividido entre episode_vwap.\n",
    "        \"\"\"\n",
    "        # TODO: Establece y devuelve un reward cuando vol == 0\n",
    "        if vol == 0:\n",
    "            reward = \"-----\"\n",
    "            return reward\n",
    "        # TODO: Calcula y devuelve el reward cuando vol > 0\n",
    "        # Clue: Utiliza episode_vwap y price para la reward por defecto\n",
    "        # Opcional: Utiliza el self y elimina los parámetros de la función\n",
    "        reward = \"-----\"\n",
    "        return reward\n",
    "    \n",
    "    def _compute_stop_conditions(self) -> Tuple[bool, bool]:\n",
    "        \"\"\"(10) Define las condiciones de parada del episodio\n",
    "        Return:\n",
    "            Tiempo agotado, orden completada\n",
    "        \"\"\"\n",
    "        # TODO: Calcula las variables de parada y devuélvelas en el orden apropiado\n",
    "        is_bins_complete = \"-----\"\n",
    "        is_ord_complete = \"-----\"\n",
    "        return is_bins_complete, is_ord_complete\n",
    "    \n",
    "    def _compute_done_reward(self) -> float:\n",
    "        \"\"\"(12)\n",
    "        \"\"\"\n",
    "        # TODO: Free style\n",
    "        done_reward = 0\n",
    "        return done_reward\n",
    "    \n",
    "    def _agg_action(self) -> float:\n",
    "        \"\"\"(7) Acción agresiva de compra de un título a precio de episode['ask1'].\n",
    "        Devolvemos el reward asociado a esa acción.\n",
    "        \"\"\"\n",
    "        # TODO: obtén el precio de la accion agresiva (ask1) en el state_pos\n",
    "        price = \"-----\"\n",
    "        # TODO: guarda price en price_hist, añade 1 a exec_vol y añade 1 a vol_hist\n",
    "        \"-----\"\n",
    "        vol = 1\n",
    "        self.exec_vol += \"-----\"\n",
    "        \"-----\"\n",
    "        \n",
    "        # TODO: utiliza la función apropiada para calcula el algo_vwap\n",
    "        algo_vwap = \"-----\"\n",
    "        # guarda el algo_vwap en algo_vwap_hist\n",
    "        \"-----\"\n",
    "        # TODO: calcula el reward utilizando la función apropiada\n",
    "        reward = \"-----\"\n",
    "        return reward\n",
    "\n",
    "    def _do_nothing(self) -> float:\n",
    "        \"\"\"(9) No hacer nada y devolvemos el reward asociado a la acción\n",
    "        \"\"\"\n",
    "        # TODO: Repite el proceso de _agg_action\n",
    "        # Clue: Precio y volumen ejecutado = 0\n",
    "        price = \"-----\"\n",
    "        vol = \"-----\"\n",
    "        \"-----\"\n",
    "        \"-----\"\n",
    "        algo_vwap = \"-----\"\n",
    "        \"-----\"\n",
    "        reward = \"-----\"\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _compute_market_vwap(self) -> float:\n",
    "        \"\"\"(5) Cálculo del VWAP del mercado hasta el step actual.\n",
    "        \"\"\"\n",
    "        # TODO: Establece un precio para el vol ejecutado por el mkt en cada step\n",
    "        # Clue: puedes fijarte en _compute_episode_market_feat\n",
    "        mid_p = \"-----\"\n",
    "        mkt_p = \"-----\"\n",
    "        # Calcula todos los vwap del mkt hasta el step actual incluido\n",
    "        v = \"-----\"\n",
    "        p_arr = \"-----\"\n",
    "        v_arr = \"-----\"\n",
    "        sum_vol = \"-----\"\n",
    "        # Si el mkt vol hasta el step == 0, devuelve el último precio hasta el step\n",
    "        if sum_vol == 0:\n",
    "            return \"-----\"\n",
    "        # Calcula y devuelve el vwap acumulado hasta el step\n",
    "        market_vwap = \"-----\"\n",
    "        return market_vwap\n",
    "    \n",
    "    def _compute_done(self) -> bool:\n",
    "        \"\"\"(11) Reglas de finalización del episodio.\n",
    "        \"\"\"\n",
    "        # TODO: Calcula las condiciones de parada utilizando la función adecuada\n",
    "        conditions = \"-----\"\n",
    "        is_bins_complete = \"-----\"\n",
    "        is_ord_complete = \"-----\"\n",
    "        done = \"-----\"\n",
    "        # TODO: Devuelve done == True si se cumplen cualquiera de las condiciones\n",
    "        return done\n",
    "\n",
    "    def step(self, action) -> Tuple[np.array, float, bool, dict]:\n",
    "        \"\"\" Evalua la acción, calcula la recompensa, devuelve el \n",
    "        nuevo estado y si el episodio ha terminado.\n",
    "        \"\"\"\n",
    "        market_vwap = self._compute_market_vwap()\n",
    "        \n",
    "        act_fn = self.actions_fn.get(action)\n",
    "        if act_fn is None:\n",
    "            raise ValueError(\n",
    "                f\"Invalid action {action}. Valid actions {self.actions_fn.keys()}\"\n",
    "            )\n",
    "\n",
    "        reward = act_fn()\n",
    "\n",
    "        self.market_vwap_hist.append(market_vwap)\n",
    "        self.reward_hist.append(reward)\n",
    "        \n",
    "        self.state_pos += 1\n",
    "        \n",
    "        done = self._compute_done()\n",
    "        \n",
    "        if done:\n",
    "            reward += self._compute_done_reward()\n",
    "            return None, reward, done, {}\n",
    "        \n",
    "        observation = self.observation_builder()\n",
    "        \n",
    "        return np.array(observation), reward, done, {}\n",
    "    \n",
    "    def action_sample(self) -> int:\n",
    "        \"\"\"(13) Devuelve una acción aleatoria. El valor ha de corresponder \n",
    "        con las keys de actions_fn.\n",
    "        \"\"\"\n",
    "        # TODO: Toma una acción aleatoria\n",
    "        # Opcional: ¿Qué distribución de prob es mejor para la exploración?\n",
    "        p = \"-----\"\n",
    "        action = \"-----\"\n",
    "        return action\n",
    "\n",
    "    def stats_df(self):\n",
    "        \"\"\"Información para el gráfico de resultados de la ejecución\n",
    "        \"\"\"\n",
    "        \n",
    "        my_df = pd.DataFrame(\n",
    "            {\"vwap\": self.algo_vwap_hist, \"vol\": self.vol_hist},\n",
    "            index=list(self.episode.index)[:len(self.algo_vwap_hist)]\n",
    "        )\n",
    "        my_df = my_df.reindex(self.episode.index)\n",
    "        my_df[\"vol\"] = my_df[\"vol\"].fillna(0)\n",
    "        my_df[\"vwap\"] = my_df[\"vwap\"].ffill()\n",
    "            \n",
    "        \n",
    "        p = self.episode[\"ask1\"]\n",
    "        v = self.episode[\"cumvol\"].diff().shift(-1)\n",
    "        last_v = self.episode_full[\"cumvol\"].diff()[-1]\n",
    "        v.iloc[-1] = last_v\n",
    "        market_vwap = (p * v).cumsum() / v.cumsum()\n",
    "        market_df = pd.DataFrame(\n",
    "            {\"vwap\": market_vwap, \"vol\": v},\n",
    "            index=v.index\n",
    "        )\n",
    "        \n",
    "        mpx = (self.episode[\"ask1\"] + self.episode[\"bid1\"]) / 2\n",
    "        \n",
    "        return my_df, market_df, mpx\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-specialist",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h2>TWAP</h2></center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "El <b><i>time weighted average price</i></b> (TWAP) es un algoritmo tradicional de negociación basado en el precio medio ponderado por el tiempo, utilizado normalmente para la ejecución de órdenes grandes para reducir el impacto de mercado. Básicamente, se trocea la orden de manera que ejecute las órdenes en un tiempo equidistante durante el tiempo de ejecución seleccionado por el cliente. Puede ser fácil adivinar el patrón de negociación de la estrategia en ejecución si sus órdenes no se modifican de una manera especial, por lo que los parámetros se pueden ajustar o alterar para hacer que la estrategia sea más difícil de rastrear. Las soluciones más comunes son la aleatorización del tamaño de las órdenes y/o el tiempo de envio entre cada una de ellas. Es posible limitar la cantidad para que no exceda un porcentaje definido del volumen de participación, o también evitar agresiones de más de una posición del libro. Esto nos ayuda a minimizar el impacto de las estrategias en el mercado.\n",
    "</p>  \n",
    "<p style=\"width:80%\">\n",
    "Para el objetivo del taller, se pide a los participantes implementar un TWAP sin la necesidad de realizar ninguna modificación. El agente TWAP ha de trocear y ejecutar las órdenes hijas de la orden CARE equidistantes en el tiempo, sin necesidad de incorporar mecanimos para la no detección\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TWAP(DDQNAgent):\n",
    "    def act(self, s):\n",
    "        # TODO: Configura un TWAP determinista utilizando s\n",
    "        a = \"-----\"\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-northeast",
   "metadata": {},
   "source": [
    "------------------\n",
    "<center><h2>Parametrización del Problema</h2></center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "Una vez definido el entorno que vamos a utilizar como simulador de mercado, necesitamos definir el agente que va a encargarse de aprender a ejecutar las órdenes CARE. El agente es el <b><i>cerebro</i></b> del sistema y puede tener, o no, inteligencia artificial implementada. Por ejemplo, la clase TWAP anteriormente implementada depende únicamente de una regla determinista. Por otro lado, los algoritmos vistos durante el bloque de Aprendizaje por Refuerzo dependerán de la construcción de un modelo de aprendizaje y, por tanto, requerirán la parametrización de estos. Los parámetros e hiperparámetros del modelo dependerán de la aproximación y modelo que elijamos.\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-league",
   "metadata": {},
   "source": [
    "---\n",
    "<center >\n",
    "<p style=\"width:80%\">\n",
    "    <b>Epsilon</b>: Parámetro de exploración para los algoritmos basados en funciones valor (por ejemplo, los deep Q-learning). Puede tomar valores entre [0,1].\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Min Epsilon</b>: Valor mínimo del epsilon por si se quier dejar aleatoriedad durante procesos de entrenamiento muy largos en los que el <b>epsilon</b> decaería hasta cero.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Espilon_decay</b>: Valor numérico mediante el cual se reduce el valor de epsilon cada determinado número de episodios.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Gamma</b>: Término de descuento para el calculo de los retornos acumulados con descuento. Dado que nuestro MDP se define con tareas episódicas gamma podrá tomar valores en el intervalo [0,1].\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Alpha</b>: Step-size, término indicando el tamaño de las actualizaciones durante el aprendizaje. Dado que nuestro problema es no estacionario (el sistema evoluciona en el tiempo) el alpha será un valor estático o adaptativo pero nunca un valor no descendente a cero en el tiempo.\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Buffer Size</b>: Tamaño de la memoria encargada de obtener un conjunto de datos que aproximadamente sea independiente e identicamente distribuido (i.i.d).\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "---\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "<b>*Nota*</b>: Los parametros que definen la arquitectura de la red dependen de lo flexible y compleja que se contruyan en los agentes. Los agentes ofrecidos para el taller mantienen la filosofia del mismo y se mantendrán sencillos.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Batch size</b>: Tamaño de los paquetes de datos que se envian al modelo durante su aprendizaje.\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Hidden_neurons</b>: Número de neuronas en las capatas ocultas de los modelos.\n",
    "</p> \n",
    "</center>\n",
    "\n",
    "---\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Nepisodes</b>: Número de sessiones de trading (episodios) que se vana realizar para el entrenamiento del modelo.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>N_log</b>: Valor numérico indicando cada cuantos episodios se realiza un log de la situación actual del aprendizaje.\n",
    "</p> \n",
    "</center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "    <b>Learn_after</b>: Valor numérico indicando cada cuantos steps se realiza un aprendizaje por parte de las redes.\n",
    "</p> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Agent Params\n",
    "\"\"\"\n",
    "# TODO: Selecciona un epsilon inicial para el entrenamiento\n",
    "epsilon = \"-----\"\n",
    "# TODO: Selecciona un min_epsilon para el entrenamiento\n",
    "min_epsilon = \"-----\"\n",
    "# TODO: Selecciona un gamma para el aprendizaje\n",
    "gamma = \"-----\"\n",
    "# TODO: Selecciona un alpha para el aprendizaje\n",
    "alpha = \"-----\"\n",
    "# TODO: Selecciona un buffer_size para el aprendizaje\n",
    "buffer_size = \"-----\"\n",
    "# TODO: Selecciona un batch_size para el aprendizaje\n",
    "batch_size = \"-----\"\n",
    "# TODO: Selecciona el número de nueronas para el modelo\n",
    "hidden_neurons = \"-----\"\n",
    "\n",
    "\"\"\"\n",
    "    Training Params\n",
    "\"\"\"\n",
    "# TODO: Selecciona el número de episodios\n",
    "nepisodes = \"-----\"\n",
    "n_log = 25\n",
    "#TODO: Determina el epsilon_decay para el proceso de entrenamiento\n",
    "epsilon_decay = \"-----\"\n",
    "learn_after = batch_size\n",
    "\n",
    "env = BestExecutionEnv(data, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-incidence",
   "metadata": {},
   "source": [
    "<center><h3>Inicialización del agente</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = \"-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-factory",
   "metadata": {},
   "source": [
    "<center><h3>Recolección de los Episodios para el Buffer</h3></center>\n",
    "<center>\n",
    "<p style=\"width:80%\">\n",
    "Antes de empezar el entrenamiento, para que el modelo tenga experiencias de las que aprender y poder tomar acciones greedy con cierto criterio ha de tener muestras para entrenar. Para ello vamos a llenar el buffer con experiencias siguendo una política estocástica con el objetivo de explorar entorno inicialmente.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto eps es 1 -> actuando random\n",
    "s = env.reset()\n",
    "for exps in range(buffer_size):  \n",
    "    a = agent.act(s)\n",
    "    s1, r, done, _ = env.step(a)\n",
    "    agent.experience(s, a, r, s1, done)\n",
    "    s = s1\n",
    "    if not exps % 10000:\n",
    "        print(f'buffer exps: {exps}')\n",
    "    if done:\n",
    "        s = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-significance",
   "metadata": {},
   "source": [
    "#### Train Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_trainable(True)\n",
    "learn_counter = 0\n",
    "history_steps = []\n",
    "history_rewards = []\n",
    "history_disc_rewards = []\n",
    "history_losses = []\n",
    "for episode in range(nepisodes):\n",
    "    s = env.reset()\n",
    "    step = 0\n",
    "    cum_reward = 0\n",
    "    dis_cum_reward = 0\n",
    "    episode_losses = []\n",
    "    while True:\n",
    "        a = agent.act(s)\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        agent.experience(s, a, r, s1, done)\n",
    "        learn_counter += 1\n",
    "        cum_reward += r\n",
    "        dis_cum_reward += agent.gamma ** step * r\n",
    "        s = s1\n",
    "        step += 1\n",
    "        if not learn_counter % learn_after:\n",
    "            mse = agent.learn()\n",
    "        if done:\n",
    "            agent.epsilon = max([agent.epsilon - epsilon_decay, min_epsilon])\n",
    "            history_rewards.append(cum_reward)\n",
    "            history_disc_rewards.append(dis_cum_reward)\n",
    "            history_losses.append(mse)\n",
    "            history_steps.append(step)\n",
    "            if not episode % n_log:\n",
    "                mse = agent.learn()\n",
    "                print(\n",
    "                    f'Episode: {episode}, '\n",
    "                    f'steps: {np.round(np.mean(history_steps[-n_log:]), 2)}, '\n",
    "                    f'rew: {np.round(np.mean(history_rewards[-n_log:]), 2)}, '\n",
    "                    f'mse: {np.round(mse)}, '\n",
    "                    f'eps: {np.round(agent.epsilon, 2)}'\n",
    "                )\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-stomach",
   "metadata": {},
   "source": [
    "#### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.set_trainable(False)\n",
    "cum_reward = 0\n",
    "step = 0\n",
    "s = env.reset()\n",
    "while True:\n",
    "    a = agent.act(s)\n",
    "    s, r, done, _ = env.step(a)\n",
    "    step += 1\n",
    "    cum_reward += agent.gamma ** step * r\n",
    "    if done:\n",
    "        break\n",
    "plot_results(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_rewards).rolling(20).mean().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
